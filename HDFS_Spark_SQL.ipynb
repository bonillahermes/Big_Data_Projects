{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonillahermes/Big_Data_Projects/blob/main/HDFS_Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hermes Yate Bonilla\n",
        "**Data Scientist**\n",
        "---\n",
        "\n",
        "**Contact:**\n",
        "- **Email:** [bonillahermes@gmail.com](mailto:bonillahermes@gmail.com)\n",
        "- **LinkedIn:** [linkedin.com/in/bonillahermes](https://www.linkedin.com/in/bonillahermes/)\n",
        "- **GitHub:** [github.com/bonillahermes](https://github.com/bonillahermes)\n",
        "- **Webpage:** [bonillahermes.com](https://bonillahermes.com/)\n",
        "---"
      ],
      "metadata": {
        "id": "OaLi2pRK3LC9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "15bfcfc880f2897e6836933d55c7c042",
          "grade": false,
          "grade_id": "cell-570cf80ae1b2c48e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2251jzZC2rsb"
      },
      "source": [
        "#HDFS, Spark SQL y MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminares"
      ],
      "metadata": {
        "id": "mEtO21v53rIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql.functions import avg, col\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import Binarizer\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "oSkmQefl3qhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "10cdca7b33ec4328e542f94942371393",
          "grade": false,
          "grade_id": "cell-42368b0202b6ce77",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-ZLIp9yo2rsg"
      },
      "source": [
        "## Leemos el fichero flights.csv que hemos subido a HDFS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2b86205b9dc8b69adf7aa9ba76518864",
          "grade": false,
          "grade_id": "cell-3202a483f423590a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "lImanww82rsg"
      },
      "source": [
        "Indicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque después comprobaremos si lo\n",
        "ha inferido correctamente o no. La ruta del archivo en HDFS debería ser /<nombre_alumno>/flights.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5310bc8c3244ee98391957d26dc91d08",
          "grade": false,
          "grade_id": "lectura-fichero",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "VbfATdfn2rsg"
      },
      "outputs": [],
      "source": [
        "#hdfs dfs -ls /user/bonil/\n",
        "#hdfs dfs -ls /user/bonil/flights.csv\n",
        "\n",
        "\n",
        "# Ruta correcta del fichero flights.csv en HDFS\n",
        "ruta_hdfs = \"/user/bonil/flights.csv\"\n",
        "\n",
        "# Creación del DataFrame a partir del archivo CSV en HDFS\n",
        "flightsDF = spark.read\\\n",
        "             .option(\"header\", \"true\")\\\n",
        "             .option(\"inferSchema\", \"true\")\\\n",
        "             .csv(ruta_hdfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRdsRjQ_2rsi"
      },
      "source": [
        "Imprimimos el esquema para comprobar qué tipo de dato ha inferido en cada columna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFLbIS9b2rsi"
      },
      "outputs": [],
      "source": [
        "flightsDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nInUWF4K2rsj"
      },
      "source": [
        "Mostramos el número de filas que tiene el DataFrame para hacernos una idea de su tamaño:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOpu7Y3f2rsj"
      },
      "outputs": [],
      "source": [
        "flightsDF.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67-z3vXB2rsk"
      },
      "source": [
        "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fNEawg32rsk"
      },
      "outputs": [],
      "source": [
        "flightsDF.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl8p25VO2rsk"
      },
      "source": [
        "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n",
        "*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\n",
        "el tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark\n",
        "las muestra como string:\n",
        "<ul>\n",
        " <li>dep_time: string (nullable = true)\n",
        " <li>dep_delay: string (nullable = true)\n",
        " <li>arr_time: string (nullable = true)\n",
        " <li>arr_delay: string (nullable = true)\n",
        " <li>air_time: string (nullable = true)\n",
        " <li>hour: string (nullable = true)\n",
        " <li>minute: string (nullable = true)    \n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isUVdZmL2rsl"
      },
      "source": [
        "Vamos a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-isAhius2rsm"
      },
      "outputs": [],
      "source": [
        "cuantos_NA = flightsDF\\\n",
        "                .where(F.col(\"dep_time\") == \"NA\")\\\n",
        "                .count()\n",
        "cuantos_NA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTR1LyYk2rsm"
      },
      "source": [
        "Por tanto, hay 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\n",
        "a la cantidad de datos que tenemos. En nuestro caso, como tenemos un número considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLWBaQnI2rsm"
      },
      "outputs": [],
      "source": [
        "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
        "\n",
        "flightsLimpiado = flightsDF\n",
        "for nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n",
        "    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
        "\n",
        "flightsLimpiado.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-47IcA7x2rsn"
      },
      "source": [
        "Si ahora mostramos el número de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\n",
        "pero sigue siendo un número considerable como para realizar analítica y sacar conclusiones sobre estos datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E293TCd2rsn"
      },
      "outputs": [],
      "source": [
        "flightsLimpiado.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo430K582rsn"
      },
      "source": [
        "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string.\n",
        "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. Vamos también a convertir la columna `arr_delay` de tipo entero a número real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlroa_9U2rsn"
      },
      "outputs": [],
      "source": [
        "flightsConvertido = flightsLimpiado\n",
        "\n",
        "for c in columnas_limpiar:\n",
        "    # método que crea una columna o reemplaza una existente\n",
        "    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType()))\n",
        "\n",
        "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\n",
        "flightsConvertido.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4Nhsh6q2rso"
      },
      "outputs": [],
      "source": [
        "flightsConvertido.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkdmVyO12rso"
      },
      "source": [
        "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya teníamos, pero ahora\n",
        "Spark sí está tratando como enteros las columnas que deberían serlo, y si queremos podemos hacer operaciones aritméticas\n",
        "con ellas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrLVOs_82rso"
      },
      "outputs": [],
      "source": [
        "flightsConvertido.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5810b869bd7baccabe0a2952dd0baae1",
          "grade": false,
          "grade_id": "cell-c0cfdd1db1edaa7d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Sw5Gksh12rso"
      },
      "source": [
        "Partiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide:\n",
        "\n",
        "* Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. ¿Cuántas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`.\n",
        "* Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cuántas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8fb2ec5d49ff84edae4833eca797068b",
          "grade": false,
          "grade_id": "ejercicio-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "SGPUJ8Lp2rsp"
      },
      "outputs": [],
      "source": [
        "# Crear aeropuertosOrigenDF con aeropuertos de origen distintos\n",
        "aeropuertosOrigenDF = flightsConvertido.select(\"origin\").distinct()\n",
        "\n",
        "# Contar el número de aeropuertos de origen distintos\n",
        "n_origen = aeropuertosOrigenDF.count()\n",
        "\n",
        "# Crear rutasDistintasDF con combinaciones únicas de origen y destino\n",
        "rutasDistintasDF = flightsConvertido.select(\"origin\", \"dest\").distinct()\n",
        "\n",
        "# Contar el número de combinaciones únicas de rutas\n",
        "n_rutas = rutasDistintasDF.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f02d6e47bb3cc84e97f31cce091f80b3",
          "grade": true,
          "grade_id": "ejercicio-1-test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "OMTOFvFj2rsp"
      },
      "outputs": [],
      "source": [
        "assert(n_origen == 2)\n",
        "assert(n_rutas == 115)\n",
        "assert(aeropuertosOrigenDF.count() == n_origen)\n",
        "assert(rutasDistintasDF.count() == n_rutas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9229f0b48ed644e35a731b404738edf2",
          "grade": false,
          "grade_id": "cell-2b5f0dea18728fcf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8jIIDMoi2rsp"
      },
      "source": [
        "### Ejercicio 2\n",
        "\n",
        "* Partiendo de nuevo de `flightsConvertido`, se pide calcular, *sólo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar **ordenado de mayor a menor retraso medio**. El código que calcule esto debería ir encapsulado en una función de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo descrito anteriormente.\n",
        "\n",
        "* Una vez hecha la función, invocarla pasándole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "097436083d0007a7d99d5108e1a504c9",
          "grade": false,
          "grade_id": "ejercicio-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mg9OOC0X2rsp"
      },
      "outputs": [],
      "source": [
        "def retrasoMedio(df):\n",
        "    # Filtrar los vuelos que llegan con retraso positivo\n",
        "    df_retraso_positivo = df.filter(col(\"arr_delay\") > 0)\n",
        "\n",
        "    # Calcular el retraso medio por aeropuerto de destino\n",
        "    df_retraso_medio = df_retraso_positivo.groupBy(\"dest\")\\\n",
        "        .agg(avg(\"arr_delay\").alias(\"retraso_medio\"))\n",
        "\n",
        "    # Ordenar por retraso medio de mayor a menor\n",
        "    df_ordenado = df_retraso_medio.orderBy(col(\"retraso_medio\").desc())\n",
        "\n",
        "    return df_ordenado\n",
        "\n",
        "# Invocar la función con flightsConvertido y almacenar el resultado\n",
        "retrasoMedioDF = retrasoMedio(flightsConvertido)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "12aa19d467a0c50adface20495b6cf35",
          "grade": true,
          "grade_id": "ejercicio-2-tests",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f8Pm5u4-2rsq"
      },
      "outputs": [],
      "source": [
        "lista = retrasoMedio(flightsConvertido).take(3)\n",
        "assert((lista[0].retraso_medio == 64.75) & (lista[0].dest == \"BOI\"))\n",
        "assert((lista[1].retraso_medio == 46.8) & (lista[1].dest == \"HDN\"))\n",
        "assert((round(lista[2].retraso_medio, 2) == 41.19) & (lista[2].dest == \"SFO\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCX45oXW2rsq"
      },
      "source": [
        "Ahora invocamos a nuestra función `retrasoMedio` pasándole como argumento `flightsConvertido`. ¿Cuáles son los tres aeropuertos con mayor retraso medio? ¿Cuáles son sus retrasos medios en minutos?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUEl1VMY2rsq"
      },
      "outputs": [],
      "source": [
        "# Mostrar los diez primeros registros de retrasoMedioDF\n",
        "retrasoMedioDF.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fa9ed55cd86eb0958e150d6a918db1af",
          "grade": false,
          "grade_id": "cell-e577747d4427e32b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "DkoowqWn2rsr"
      },
      "source": [
        "### Ejercicio 3\n",
        "\n",
        "Ajustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d4405b7db7180445df5cacc66d82db53",
          "grade": false,
          "grade_id": "cell-e577747d4427e32a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "__e6r6wI2rsr"
      },
      "source": [
        "Notemos que en estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Debemos indicar a Spark cuáles son categóricas e indexarlas. Para ello se pide:\n",
        "\n",
        "* Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categóricas `month` y `carrier` (tipo de avión). El nombre de las columnas indexadas que se crearán debe ser, respectivamente, `monthIndexed` y `carrierIndexed`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "eb039584cd14e6bc3434e5be930341e6",
          "grade": false,
          "grade_id": "string-indexer",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ca4sPsKq2rss"
      },
      "outputs": [],
      "source": [
        "# Crear StringIndexer para la columna 'month'\n",
        "indexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\")\n",
        "\n",
        "# Crear StringIndexer para la columna 'carrier'\n",
        "indexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c96c7c7d5836922dd549b358b897b781",
          "grade": true,
          "grade_id": "string-indexer-tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5n8dZaeK2rss"
      },
      "outputs": [],
      "source": [
        "assert(isinstance(indexerMonth, StringIndexer))\n",
        "assert(isinstance(indexerCarrier, StringIndexer))\n",
        "assert(indexerMonth.getInputCol() == \"month\")\n",
        "assert(indexerMonth.getOutputCol() == \"monthIndexed\")\n",
        "assert(indexerCarrier.getInputCol() == \"carrier\")\n",
        "assert(indexerCarrier.getOutputCol() == \"carrierIndexed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "69e024b0baeeb36bb74d136c7e113372",
          "grade": false,
          "grade_id": "cell-e577747d4427e323",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "i5I4DHxe2rss"
      },
      "source": [
        "Recordemos también que Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n",
        "\n",
        "* Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que serán las que formarán parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es lógico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas debe llamarse `features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
          "grade": false,
          "grade_id": "vector-assembler",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "b0k5VhH92rss"
      },
      "outputs": [],
      "source": [
        "# Lista de columnas para ensamblar, incluyendo las versiones indexadas de las categóricas\n",
        "columnas_ensamblar = [\"monthIndexed\", \"carrierIndexed\", \"day\", \"dep_time\", \"arr_time\", \"distance\", \"air_time\"]\n",
        "\n",
        "# Crear el VectorAssembler\n",
        "vectorAssembler = VectorAssembler(inputCols=columnas_ensamblar, outputCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0d17b2fa1d8a4bd02b89952429ba1552",
          "grade": true,
          "grade_id": "vector-assembler-tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "l07-Zpna2rst"
      },
      "outputs": [],
      "source": [
        "assert(isinstance(vectorAssembler, VectorAssembler))\n",
        "assert(vectorAssembler.getOutputCol() == \"features\")\n",
        "input_cols = vectorAssembler.getInputCols()\n",
        "assert(len(input_cols) == 7)\n",
        "assert(\"arr_delay\" not in input_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8eeb3a3f0b15e8b374789706cd9bce49",
          "grade": false,
          "grade_id": "cell-e577747d4427e32dsdf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "48FYUQBT2rs3"
      },
      "source": [
        "Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n",
        "\n",
        "* Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
          "grade": false,
          "grade_id": "binarizer",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1aIl6HUj2rs3"
      },
      "outputs": [],
      "source": [
        "# Crear el Binarizer con un umbral de 15 minutos\n",
        "delayBinarizer = Binarizer(threshold=15.0, inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7ec408ab263c0378526f96bb5d374704",
          "grade": true,
          "grade_id": "binarizer-tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ww3LpwKQ2rs3"
      },
      "outputs": [],
      "source": [
        "assert(isinstance(delayBinarizer, Binarizer))\n",
        "assert(delayBinarizer.getThreshold() == 15)\n",
        "assert(delayBinarizer.getInputCol() == \"arr_delay\")\n",
        "assert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5aed16d7ed0219fe1d8741848f594319",
          "grade": false,
          "grade_id": "cell-25a7793978ee7d05",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "AOMV5U-62rs4"
      },
      "source": [
        "Por último, crearemos el modelo de clasificación.\n",
        "\n",
        "* Crear en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n",
        "* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n",
        "* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e785136d6d06691c003ff9542027e03d",
          "grade": false,
          "grade_id": "decision-tree",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "EUf0TAyW2rs4"
      },
      "outputs": [],
      "source": [
        "# Crear el modelo de árbol de decisión\n",
        "decisionTree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"arr_delay_binary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2b03b48f304b5b34cd06eeec49e001fd",
          "grade": true,
          "grade_id": "decision-tree-tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LIGkf68p2rs4"
      },
      "outputs": [],
      "source": [
        "assert(isinstance(decisionTree, DecisionTreeClassifier))\n",
        "assert(decisionTree.getFeaturesCol() == \"features\")\n",
        "assert(decisionTree.getLabelCol() == \"arr_delay_binary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "728da91edabbac5b62baf31bdd0a707e",
          "grade": false,
          "grade_id": "cell-e577747d4427e32d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zuDa4fhD2rs4"
      },
      "source": [
        "Ahora vamos a encapsular todas las fases en un sólo pipeline y procederemos a entrenarlo. Se pide:\n",
        "\n",
        "* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo.\n",
        "\n",
        "* Entrenarlo invocando sobre ella al método `fit` y guardar el pipeline entrenado devuelto por dicho método en una variable llamada `pipelineModel`.\n",
        "\n",
        "* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. Nótese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no habíamos hecho (aunque habría sido lo correcto) ninguna división de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "edbdb627305d03efa41a88426330e160",
          "grade": false,
          "grade_id": "pipeline",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_k5D9w4R2rs5"
      },
      "outputs": [],
      "source": [
        "# Crear el Pipeline con todas las etapas\n",
        "pipeline = Pipeline(stages=[indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree])\n",
        "\n",
        "# Ajustar el Pipeline al DataFrame flightsConvertido\n",
        "pipelineModel = pipeline.fit(flightsConvertido)\n",
        "\n",
        "# Aplicar el Pipeline entrenado para hacer predicciones\n",
        "flightsPredictions = pipelineModel.transform(flightsConvertido)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "107b5ec260550eb4235ffecff655289a",
          "grade": true,
          "grade_id": "pipeline-tests",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "88GMlwvy2rs5"
      },
      "outputs": [],
      "source": [
        "assert(isinstance(pipeline, Pipeline))\n",
        "assert(len(pipeline.getStages()) == 5)\n",
        "assert(isinstance(pipelineModel, PipelineModel))\n",
        "assert(\"probability\" in flightsPredictions.columns)\n",
        "assert(\"prediction\" in flightsPredictions.columns)\n",
        "assert(\"rawPrediction\" in flightsPredictions.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0c531953abf18cfb3b67571ddde7a57d",
          "grade": false,
          "grade_id": "cell-61156fe5938763f1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1kGyTYQd2rs5"
      },
      "source": [
        "Vamos a mostrar la matriz de confusión (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
          "grade": false,
          "grade_id": "cell-896752beb71cb455",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "uBUUnpmi2rs5"
      },
      "outputs": [],
      "source": [
        "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}